from __future__ import annotations

import os
import sys
import warnings
import math
import logging
from functools import partial
import re
import string
import torch

from ..base import BaseModel
from .prompt import Qwen2VLPromptMixin
from ...smp import get_rank_and_world_size, get_gpu_memory, listinstr
from ...dataset import DATASET_MODALITY



def modify_question_options(original_prompt: str, exclude_options: list[str]) -> tuple[str, dict[str, str]]:
    """
    Removes specified options from a multiple-choice question string,
    re-labels the remaining options starting from 'A', and returns
    the modified prompt string along with a mapping from new labels to original labels.

    Args:
        original_prompt: The original string containing the question and options.
        exclude_options: A list of original option labels to remove (e.g., ['C', 'D']).

    Returns:
        A tuple containing:
        - modified_prompt (str): The prompt with specified options removed and
                                 remaining options re-labeled.
        - new_to_original_map (dict): A dictionary mapping the new option labels
                                      (e.g., 'A', 'B') back to their original
                                      labels (e.g., {'A': 'A', 'B': 'C'}).
    """
    # Normalize exclude_options to uppercase
    exclude_options = [opt.upper() for opt in exclude_options]

    # Regex to find options (A, B, C, D), capturing label and text.
    # Handles potential whitespace and multi-line option text.
    # Stops before the next option or the "Answer:" part.
    option_pattern = re.compile(
        r"^([ABCD])\.\s*(.*?)(?=\n[ABCD]\.|\nAnswer:|\Z)",
        re.MULTILINE | re.DOTALL | re.IGNORECASE
    )

    matches = list(option_pattern.finditer(original_prompt))

    if not matches:
        # No options found, return original prompt and empty map
        return original_prompt, {}

    original_options_data = []
    for match in matches:
        original_options_data.append({
            "label": match.group(1).upper(),
            "text": match.group(2).strip(),
            "start": match.start(),
            "end": match.end()
        })

    # Filter out excluded options
    remaining_options = [
        opt for opt in original_options_data if opt["label"] not in exclude_options
    ]

    if not remaining_options:
         # Handle case where all options are excluded (optional: raise error or return specific state)
         print("Warning: All options were excluded.")
         # Return prompt up to where options started, plus "Answer:", and empty map
         options_start_index = original_options_data[0]['start']
         prefix = original_prompt[:options_start_index].strip()
         return prefix + "\nAnswer:", {}


    # Re-label remaining options and build the mapping
    new_options_list = []
    new_to_original_map = {}
    new_labels = "ABCDEFGHIJKLMNOPQRSTUVWXYZ" # Sufficient labels

    for i, option_data in enumerate(remaining_options):
        new_label = new_labels[i]
        new_options_list.append(f"{new_label}. {option_data['text']}")
        new_to_original_map[new_label] = option_data['label']

    # Reconstruct the prompt string
    # Find the start of the first option and the start of the "Answer:" section
    options_start_index = original_options_data[0]['start']
    # Find where the original options block ends (end of the last original match)
    options_end_index = original_options_data[-1]['end']

    prefix = original_prompt[:options_start_index].strip()
    # Find the "Answer:" part, assuming it comes after the options
    answer_part_match = re.search(r"\nAnswer:", original_prompt[options_end_index:], re.IGNORECASE)
    suffix = "\nAnswer:" # Default suffix if the original wasn't found after options

    if answer_part_match:
         # If original "Answer:" exists, use it to ensure correct spacing/structure
         # We deliberately exclude the "After consideration..." part here.
         suffix = "\nAnswer:" # Keep it clean

    modified_prompt = prefix + "\n" + "\n".join(new_options_list) + suffix

    return modified_prompt, new_to_original_map


def map_answer_to_original(generated_answer: str, new_to_original_map: dict[str, str]) -> str:
    """
    Maps a generated answer (corresponding to a re-labeled option) back
    to its original option label using the provided mapping.

    Args:
        generated_answer: The answer label generated by the model (e.g., 'A', 'B').
        new_to_original_map: The dictionary mapping new labels back to original labels,
                             as returned by modify_question_options.

    Returns:
        The original option label corresponding to the generated answer.
        Returns the generated_answer itself if it's not found in the map (fallback).
    """
    # Clean up potential extra characters around the answer
    cleaned_answer = generated_answer.strip().upper()
    # Basic extraction if it's like "(A)" or "A."
    match = re.match(r'\(?([A-Z])\)?\.?', cleaned_answer)
    if match:
        cleaned_answer = match.group(1)

    return new_to_original_map.get(cleaned_answer, generated_answer) # Use .get for safe fallback


def extract_multiple_choice_answer(llm_output: str, options: str = "ABCD") -> str | None:
    """
    从大模型的输出文本中提取多项选择题的答案 (A, B, C, D 等)。

    考虑到 LLM 可能的输出格式，例如：
    - "A"
    - "B."
    - "答案是 C"
    - "选择 D"
    - "The correct option is A."
    - "I choose B"
    - "**C**"
    - "(D)"
    - "A. ..." (只取 A)
    - "我觉得是 B"
    - "选项 C 是正确的"
    - 等等

    Args:
        llm_output (str): 大模型生成的包含答案的文本。
        options (str): 可能的选项字母字符串，默认为 "ABCD"。

    Returns:
        str | None: 提取到的选项字母 (大写)，如果未能明确提取则返回 None。
    """
    if not llm_output:
        return None

    # 0. 预处理：去除首尾空格，统一处理大小写可能有助于匹配，但选项本身需要区分大小写，所以暂时只strip
    text = llm_output.strip()
    if not text:
        return None

    # 1. 优先匹配明确的答案指示词 + 选项 (修正版)
    #    例如："答案是 A", "选择 B", "Option: C", "选 D", "Answer is D"
    #    (?:...) 表示非捕获组
    #    \s*[:：]?\s* 匹配可选的空格、可选的冒号（中英文）、可选的空格
    #    (?:是|is)?\s* 匹配可选的 "是" 或 "is"、可选的空格
    #    ([{options}]) 捕获选项字母
    #    \b 确保选项字母是独立的单词边界（防止匹配单词内部的字母）
    pattern_explicit = rf"(?:答案|选项|选择|answer|option|choice|choose|select)\s*[:：]?\s*(?:是|is)?\s*([{options}])\b"
    match = re.search(pattern_explicit, text, re.IGNORECASE)
    if match:
        # 提取捕获组 1 (选项字母) 并转为大写
        return match.group(1).upper()

    # 2. 匹配被括号、星号等包围的单个选项字母
    #    例如："(A)", "[B]", "**C**", "*D*", "{A}"
    #    (?<!\w) 和 (?!\w) 用于确保选项字母前后不是其他字母数字字符（比 \b 更灵活）
    #    (?:...) 包裹各种可能的包围符号
    pattern_bracketed = rf"(?:[\[({{*<])\s*([{options}])\s*(?:[\])}}*>.])"
    match = re.search(pattern_bracketed, text, re.IGNORECASE)
    if match:
        return match.group(1).upper()

    # 尝试宽松一点，匹配前后有非字母数字字符或边界的单个选项
    # 改进：使用更严格的边界检查，避免匹配 "Grade A"
    # 要求前面是开头、空格或特定标点，后面是结尾、空格或特定标点
    pattern_isolated_boundary = rf"(?:^|\s|[:：(\[{{*<])\s*([{options}])\s*(?:$|\s|[.,!?)\]}}*>:])"
    match = re.search(pattern_isolated_boundary, text, re.IGNORECASE)
    if match:
        return match.group(1).upper()

    # 3. 匹配以选项字母 + 点/括号/空格 开头的字符串
    #    例如："A. xxx", "B) yyy", "C zzz"
    #    ^ 表示字符串开头
    #    \s* 匹配开头的可选空格
    #    使用 \b 确保选项字母后不是其他字母 (防止匹配 "Apple")
    pattern_start = rf"^\s*([{options}])\b(?:\.|\)|\s|:|$)"
    match = re.match(pattern_start, text, re.IGNORECASE) # 使用 match 确保从头开始
    if match:
        return match.group(1).upper()

    # 4. 匹配句子中 "is A", "是 B" 这样的结构 (优先级较低)
    #    确保 "is" 或 "是" 前面是一个单词边界
    pattern_is_option = rf"\b(?:is|是)\s+([{options}])\b"
    match = re.search(pattern_is_option, text, re.IGNORECASE)
    if match:
        return match.group(1).upper()

    # 5. 最后手段：检查整个字符串是否仅仅是选项字母（可能带标点）
    #    去除所有标点和空格后，看是否只剩下一个在 options 中的字母
    cleaned_text = ''.join(c for c in text if c not in string.punctuation and not c.isspace())
    if len(cleaned_text) == 1 and cleaned_text.upper() in options.upper():
         return cleaned_text.upper()

    # 6. 如果以上策略都失败，则无法确定答案
    return None


VLLM_MAX_IMAGE_INPUT_NUM = 24


def ensure_image_url(image: str) -> str:
    prefixes = ['http://', 'https://', 'file://', 'data:image;']
    if any(image.startswith(prefix) for prefix in prefixes):
        return image
    if os.path.exists(image):
        return 'file://' + image
    raise ValueError(f'Invalid image: {image}')


def ensure_video_url(video: str) -> str:
    prefixes = ['http://', 'https://', 'file://', 'data:video;']
    if any(video.startswith(prefix) for prefix in prefixes):
        return video
    if os.path.exists(video):
        return 'file://' + video
    raise ValueError(f'Invalid video: {video}')


def create_image_content(image_path, min_pixels, max_pixels):
    base64_image, mime_type = encode_image(image_path)
    return {
        "type": "image",
        "image": f"data:{mime_type};base64,{base64_image}",
        'min_pixels': min_pixels,
        'max_pixels': max_pixels
    }


def encode_image(image_path, max_side=None):
    from mimetypes import guess_type
    mime_type, _ = guess_type(image_path)
    if mime_type is None:
        mime_type = "image/jpeg"
    image_format = mime_type.split("/")[-1].upper() if mime_type else "JPEG"

    from PIL import Image
    image = Image.open(image_path)
    # Handle the alpha channel
    if image.mode == "RGBA":
        image = _rgba_to_rgb(image)
    if max_side:
        image = _resize_image(image, max_side)
    encoded_image = _encode_image(image, image_format)

    return encoded_image, mime_type


def _encode_image(image, image_format):
    from io import BytesIO
    with BytesIO() as output:
        image.convert("RGB").save(output, format=image_format)
        import base64
        base64_encoded_data = base64.b64encode(output.getvalue()).decode("utf-8")
    return base64_encoded_data


def _rgba_to_rgb(image):
    from PIL import Image
    background = Image.new("RGBA", image.size, (255, 255, 255, 255))
    return Image.alpha_composite(background, image).convert("RGB")


def _resize_image(image, max_side):
    resize_scale = max_side / max(image.size)
    new_size = (
        int(image.size[0] * resize_scale),
        int(image.size[1] * resize_scale),
    )
    return image.resize(new_size)


def process_video(video_path, num_frames, min_pixels, max_pixels):
    import cv2
    # Open the video file
    cap = cv2.VideoCapture(video_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)  # Frames per second

    # the sampling rate using max number of frames
    sampling_gap_maxframe = (
        1 if not num_frames else math.ceil(frame_count / num_frames)
    )
    sampling_gap = max(math.ceil(fps / 5), sampling_gap_maxframe)

    frame_number = 0
    images = []

    while True:
        import tempfile
        success, frame = cap.read()
        if not success:
            break
        # Sample frames based on the dynamic sampling rate
        if frame_number % sampling_gap == 0:
            # Create a temporary file for the frame
            with tempfile.NamedTemporaryFile(
                suffix=".jpg", delete=False
            ) as temp_frame:
                cv2.imwrite(temp_frame.name, frame)
                images.append(create_image_content(temp_frame.name, min_pixels, max_pixels))
                os.remove(temp_frame.name)
        frame_number += 1
    if frame_number == 0:
        raise ValueError(f"Failed to read video from {video_path}, check data...")
    logging.info(
        f"Sampled {len(images)}/{frame_number} frames from video {video_path}"
    )
    cap.release()
    return images


def setup_visible_devices_per_rank():
    total_gpus = torch.cuda.device_count()
    rank, world_size = get_rank_and_world_size()
    assert world_size == 1, "Only support world_size == 1 for vLLM inference"
    num_gpus = total_gpus // world_size
    start_idx = rank * num_gpus
    assigned_devices = list(range(start_idx, start_idx + num_gpus))
    os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(str(i) for i in assigned_devices)
    logging.info(f"[Rank {rank}] Visible GPUs: {assigned_devices}")
    return num_gpus


class Qwen2VLChat_ICL(Qwen2VLPromptMixin, BaseModel):
    INSTALL_REQ = False
    INTERLEAVE = True
    VIDEO_LLM = True

    def __init__(
        self,
        model_path: str = "/mnt/afs/wangkaibin/models/Qwen2.5-VL-7B-Instruct",
        min_pixels: int | None = None,
        max_pixels: int | None = None,
        total_pixels: int | None = None,
        max_new_tokens=2048,
        top_p=0.001,
        top_k=1,
        temperature=0.01,
        repetition_penalty=1.0,
        use_custom_prompt: bool = True,
        system_prompt: str | None = None,
        post_process: bool = False,  # if True, will try to only extract stuff in the last \boxed{}.
        verbose: bool = False,
        use_audio_in_video: bool = False,
        model_config = None,
        **kwargs,
    ):
        super().__init__(use_custom_prompt=use_custom_prompt)
        self.min_pixels = min_pixels
        self.max_pixels = max_pixels
        self.total_pixels = total_pixels
        self.max_new_tokens = max_new_tokens
        if self.total_pixels and self.total_pixels > 24576 * 28 * 28:
            print('The total number of video tokens might become too large, resulting in an overly long input sequence. We recommend lowering **total_pixels** to below **24576 × 28 × 28**.')  # noqa: E501
        self.generate_kwargs = dict(
            max_new_tokens=self.max_new_tokens,
            top_p=top_p,
            top_k=top_k,
            temperature=temperature,
            repetition_penalty=repetition_penalty,
        )
        self.system_prompt = system_prompt
        self.verbose = verbose
        self.post_process = post_process
        self.fps = kwargs.pop('fps', 2)
        self.nframe = kwargs.pop('nframe', 128)
        if self.fps is None and self.nframe is None:
            print("Warning: fps and nframe are both None, \
                  using default nframe/fps setting in qwen-vl-utils/qwen-omni-utils, \
                  the fps/nframe setting in video dataset is omitted")
        self.use_audio_in_video = use_audio_in_video
        self.FRAME_FACTOR = 2
        rank, world_size = get_rank_and_world_size()
        assert model_path is not None
        self.model_path = model_path
        MODEL_CLS = None
        self.model_config = model_config

        if listinstr(['omni'], model_path.lower()):
            try:
                from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
            except Exception as err:
                logging.critical("pip install git+https://github.com/huggingface/transformers@3a1ead0aabed473eafe527915eea8c197d424356")  # noqa: E501
                raise err
            MODEL_CLS = Qwen2_5OmniForConditionalGeneration
            self.processor = Qwen2_5OmniProcessor.from_pretrained(model_path)
        elif listinstr(['2.5', '2_5', 'qwen25'], model_path.lower()):
            from transformers import AutoProcessor
            from transformers import Qwen2_5_VLForConditionalGeneration
            self.processor = AutoProcessor.from_pretrained(model_path)
            MODEL_CLS = Qwen2_5_VLForConditionalGeneration
        else:
            from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
            MODEL_CLS = Qwen2VLForConditionalGeneration
            self.processor = Qwen2VLProcessor.from_pretrained(model_path)

        gpu_mems = get_gpu_memory()
        max_gpu_mem = max(gpu_mems) if gpu_mems != [] else -1
        assert max_gpu_mem > 0
        self.use_vllm = kwargs.get('use_vllm', False)
        self.limit_mm_per_prompt = VLLM_MAX_IMAGE_INPUT_NUM
        self.model = MODEL_CLS.from_pretrained(
            model_path,
            torch_dtype='auto',
            # device_map="auto",
            device_map=0,  # btnkij
            attn_implementation='flash_attention_2',
            # model_config=model_config,
        )
        self.model.eval()

        torch.cuda.empty_cache()

    def _prepare_content(self, inputs: list[dict[str, str]], dataset: str | None = None) -> list[dict[str, str]]:
        """
        inputs list[dict[str, str]], each dict has keys: ['type', 'value']
        """
        content = []
        for s in inputs:
            if s['type'] == 'image':
                item = {'type': 'image', 'image': ensure_image_url(s['value'])}
                if dataset == 'OCRBench':
                    item['min_pixels'] = 10 * 10 * 28 * 28
                    warnings.warn(f"OCRBench dataset uses custom min_pixels={item['min_pixels']}")
                    if self.max_pixels is not None:
                        item['max_pixels'] = self.max_pixels
                else:
                    if self.min_pixels is not None:
                        item['min_pixels'] = self.min_pixels
                    if self.max_pixels is not None:
                        item['max_pixels'] = self.max_pixels
                if self.total_pixels is not None:
                    item['total_pixels'] = self.total_pixels
            elif s['type'] == 'video':
                item = {
                    'type': 'video',
                    'video': ensure_video_url(s['value'])
                }
                if self.min_pixels is not None:
                    item['min_pixels'] = self.min_pixels
                if self.max_pixels is not None:
                    item['max_pixels'] = self.max_pixels
                if self.total_pixels is not None:
                    item['total_pixels'] = self.total_pixels
                if self.fps is not None:
                    item['fps'] = self.fps
                elif self.nframe is not None:
                    import cv2
                    video = cv2.VideoCapture(s['value'])
                    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
                    video.release()
                    if frame_count < self.nframe:
                        new_frame_count = frame_count // self.FRAME_FACTOR * self.FRAME_FACTOR
                        print(f"use {new_frame_count} for {s['value']}")
                        item['nframes'] = new_frame_count
                    else:
                        item['nframes'] = self.nframe
            elif s['type'] == 'text':
                item = {'type': 'text', 'text': s['value']}
            elif s['type'] == 'audio':
                item = {'type':'audio','audio':s['value']}
            else:
                raise ValueError(f"Invalid message type: {s['type']}, {s}")
            content.append(item)
        return content

    def _prepare_content_vllm(self, inputs: list[dict[str, str]], dataset: str | None = None) -> list[dict[str, str]]:
        """
        inputs list[dict[str, str]], each dict has keys: ['type', 'value']
        """
        content = []
        video_inputs = [s for s in inputs if s['type'] == 'video']
        video_count = len(video_inputs)
        cur_image_count = 0
        for s in inputs:
            if s['type'] == 'image':
                item = {'type': 'image', 'image': ensure_image_url(s['value'])}
                if dataset == 'OCRBench':
                    item['min_pixels'] = 10 * 10 * 28 * 28
                    warnings.warn(f"OCRBench dataset uses custom min_pixels={item['min_pixels']}")
                    if self.max_pixels is not None:
                        item['max_pixels'] = self.max_pixels
                else:
                    if self.min_pixels is not None:
                        item['min_pixels'] = self.min_pixels
                    if self.max_pixels is not None:
                        item['max_pixels'] = self.max_pixels
                if self.total_pixels is not None:
                    item['total_pixels'] = self.total_pixels
                if cur_image_count < self.limit_mm_per_prompt:
                    content.append(item)
                    cur_image_count += 1
                else:
                    logging.warning(
                        f"Number of images exceeds the limit of {self.limit_mm_per_prompt}. "
                        f"Only the first {self.limit_mm_per_prompt} images will be used."
                    )
            elif s['type'] == 'video':
                if video_count > 1:
                    logging.warning(
                        "Multiple videos detected. Using video frames for each video"
                    )
                    if dataset == 'OCRBench':
                        min_pixels = 10 * 10 * 28 * 28
                        warnings.warn(f"OCRBench dataset uses custom min_pixels={min_pixels}")
                        if self.max_pixels is not None:
                            max_pixels = self.max_pixels
                    else:
                        if self.min_pixels is not None:
                            min_pixels = self.min_pixels
                        if self.max_pixels is not None:
                            max_pixels = self.max_pixels
                    import cv2
                    video = cv2.VideoCapture(s['value'])
                    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
                    video.release()

                    frames_per_video = max(1, self.limit_mm_per_prompt // video_count)
                    content.append({"type": "text", "text": "<video frames start>"})
                    content.extend(process_video(s['value'], frames_per_video, min_pixels, max_pixels))
                    content.append({"type": "text", "text": "<video frames end>"})

                else:
                    item = {
                        'type': 'video',
                        'video': ensure_video_url(s['value'])
                    }
                    if self.min_pixels is not None:
                        item['min_pixels'] = self.min_pixels
                    if self.max_pixels is not None:
                        item['max_pixels'] = self.max_pixels
                    if self.total_pixels is not None:
                        item['total_pixels'] = self.total_pixels
                    if self.fps is not None:
                        item['fps'] = self.fps
                    elif self.nframe is not None:
                        import cv2
                        video = cv2.VideoCapture(s['value'])
                        frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
                        video.release()
                        if frame_count < self.nframe:
                            new_frame_count = frame_count // self.FRAME_FACTOR * self.FRAME_FACTOR
                            print(f"use {new_frame_count} for {s['value']}")
                            item['nframes'] = new_frame_count
                        else:
                            item['nframes'] = self.nframe
                    content.append(item)
            elif s['type'] == 'text':
                item = {'type': 'text', 'text': s['value']}
                content.append(item)
            else:
                raise ValueError(f"Invalid message type: {s['type']}, {s}")
        return content

    def generate_inner_transformers(self, message, dataset=None):
        try:
            from qwen_vl_utils import process_vision_info
        except Exception as err:
            logging.critical("qwen_vl_utils not found, please install it via 'pip install qwen-vl-utils'")  # noqa: E501
            raise err

        messages = []
        if self.system_prompt is not None:
            messages.append({'role': 'system', 'content': self.system_prompt})
        messages.append({'role': 'user', 'content': self._prepare_content(message, dataset=dataset)})
        if self.verbose:
            print(f'\033[31m{messages}\033[0m')

        text = self.processor.apply_chat_template([messages], tokenize=False, add_generation_prompt=True)
        images, videos = process_vision_info([messages])
        
        def generate_chunk_answer(text, images, videos, black_list=None):
            inputs = self.processor(text=text, images=images, videos=videos, padding=True, return_tensors='pt')  # noqa: E501
            inputs = inputs.to('cuda')
            allowed_next_words = ["A", "B", "C", "D", " A", " B", " C", " D"]
            allowed_next_token_ids = self.processor(text=allowed_next_words)['input_ids']
            outputs = self.model.generate(
                **inputs,
                do_sample=False,
                temperature=0,
                max_new_tokens=1,
                # **self.generate_kwargs,
                output_scores=True,
                return_dict_in_generate=True
            )
            prob = torch.zeros(4, dtype=torch.float32, device=inputs['input_ids'].device)
            for i in range(len(allowed_next_words)):
                prob[i % 4] += outputs.scores[0][0, allowed_next_token_ids[i][-1]]
            if black_list is not None:
                prob[-len(black_list):] = -torch.inf
            choice = torch.argmax(prob)
            answer = chr(ord('A') + choice)
            # print('### black_list', black_list)
            # print('### prob', prob)
            # print('### answer', answer)
            # breakpoint()
            return answer
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
            ]
            out = self.processor.tokenizer.batch_decode(
                generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            response = out[0]
            return response

        frame_per_chunk = self.model_config.frame_per_chunk
        chunk_id = self.model_config.chunk_id
        chunk_begin = chunk_id * frame_per_chunk
        chunk_end = chunk_begin + frame_per_chunk
        video_chunk = [videos[0][chunk_begin:chunk_end]]
        answer = generate_chunk_answer(text[:], images, video_chunk)
        return answer

        frame_per_chunk = self.model_config.frame_per_chunk
        vote = {key: 0 for key in "ABCD"}
        for chunk_begin in range(0, len(videos[0]), frame_per_chunk):
            chunk_end = min(len(videos[0]), chunk_begin + frame_per_chunk)
            video_chunk = [videos[0][chunk_begin:chunk_end]]
            answer = generate_chunk_answer(text[:], images, video_chunk)
            answer = extract_multiple_choice_answer(answer)
            if answer is not None:
                vote[answer] += 1
        answer = generate_chunk_answer(text[:], images, videos)
        answer = extract_multiple_choice_answer(answer)
        if answer is not None:
            vote[answer] += 1

        exclude_answers = [key for key, value in vote.items() if value == 0]
        text, mapping = modify_question_options(text[0], exclude_answers)
        response = generate_chunk_answer([text], images, videos, black_list=exclude_answers)
        answer = extract_multiple_choice_answer(response)
        if answer is None:
            print("#### answere is None:", text, response)
            answer = "A"
        answer = map_answer_to_original(answer, mapping)

        if self.verbose:
            print(f'\033[32m{answer}\033[0m')
        return answer

    def generate_inner_vllm(self, message, dataset=None):
        from vllm import SamplingParams

        if listinstr(['omni'], self.model_path.lower()):
            try:
                from qwen_omni_utils import process_mm_info
            except Exception as err:
                logging.critical("qwen_omni_utils not found, please install it via 'pip install qwen-omni-utils[decord]'")  # noqa: E501
                raise err
        else:
            try:
                from qwen_vl_utils import process_vision_info
            except Exception as err:
                logging.critical("qwen_vl_utils not found, please install it via 'pip install qwen-vl-utils'")  # noqa: E501
                raise err

        messages = []
        if self.system_prompt is not None:
            messages.append({'role': 'system', 'content': self.system_prompt})
        messages.append({'role': 'user', 'content': self._prepare_content_vllm(message, dataset=dataset)})
        if self.verbose:
            print(f'\033[31m{messages}\033[0m')

        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        if listinstr(['omni'], self.model_path.lower()):
            audios, images, videos = process_mm_info(messages, use_audio_in_video=self.use_audio_in_video)
        else:
            images, videos = process_vision_info(messages)
        print('finishing process vision info in vllm.')

        if DATASET_MODALITY(dataset) == 'VIDEO':
            assert len(videos) == 1
            videos_nd = [videos[0].detach().cpu().numpy().transpose(0, 2, 3, 1)]

            video_inputs = {
                "prompt": text[0],
                "multi_modal_data": {"video": videos_nd[0]},
                "mm_processor_kwargs":{}
            }
            if self.use_audio_in_video:
                import vllm
                assert not vllm.envs.VLLM_USE_V1, ("V1 does not support use_audio_in_video. Please launch this example with `VLLM_USE_V1=0`.")  # noqa: E501
                video_inputs["multi_modal_data"]["audio"] = audios[0]
                video_inputs['mm_processor_kwargs']['use_audio_in_video'] = True
            if videos_nd[0].shape[0] > VLLM_MAX_IMAGE_INPUT_NUM:
                print('video input sequence may be too long for vllm, Maybe cannot generate response for VLLM')
        sampling_params = SamplingParams(
            temperature=0.0, max_tokens=self.max_new_tokens, stop_token_ids=None
        )
        if images:
            outputs = self.llm.generate(
                {
                    "prompt": text,
                    "multi_modal_data": {"image": images},
                },
                sampling_params=sampling_params,
            )
        elif videos_nd:
            outputs = self.llm.generate(
                video_inputs,
                sampling_params=sampling_params,
            )
        else:
            outputs = self.llm.generate(
                {
                    "prompt": text,
                },
                sampling_params=sampling_params,
            )

        for o in outputs:
            generated_text = o.outputs[0].text

        if self.post_process:
            resp = generated_text.split('\\boxed{')[-1]
            lt = len(resp)
            counter, end = 1, None
            for i in range(lt):
                if resp[i] == '{':
                    counter += 1
                elif resp[i] == '}':
                    counter -= 1
                if counter == 0:
                    end = i
                    break
                elif i == lt - 1:
                    end = lt
                    break
            if end is not None:
                generated_text = resp[:end]

        if self.verbose:
            print(f'\033[32m{generated_text}\033[0m')
        return generated_text

    def generate_inner(self, message, dataset=None):
        if self.use_vllm:
            return self.generate_inner_vllm(message, dataset=dataset)
        else:
            return self.generate_inner_transformers(message, dataset=dataset)
