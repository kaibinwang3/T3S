import torch
from PIL import Image
from ..base import BaseModel
from ...smp import *
from ...dataset import DATASET_TYPE, DATASET_MODALITY
import copy
import re
import string


def modify_question_options(original_prompt: str, exclude_options: list[str]) -> tuple[str, dict[str, str]]:
    """
    Removes specified options from a multiple-choice question string,
    re-labels the remaining options starting from 'A', and returns
    the modified prompt string along with a mapping from new labels to original labels.

    Args:
        original_prompt: The original string containing the question and options.
        exclude_options: A list of original option labels to remove (e.g., ['C', 'D']).

    Returns:
        A tuple containing:
        - modified_prompt (str): The prompt with specified options removed and
                                 remaining options re-labeled.
        - new_to_original_map (dict): A dictionary mapping the new option labels
                                      (e.g., 'A', 'B') back to their original
                                      labels (e.g., {'A': 'A', 'B': 'C'}).
    """
    # Normalize exclude_options to uppercase
    exclude_options = [opt.upper() for opt in exclude_options]

    # Regex to find options (A, B, C, D), capturing label and text.
    # Handles potential whitespace and multi-line option text.
    # Stops before the next option or the "Answer:" part.
    option_pattern = re.compile(
        r"^([ABCD])\.\s*(.*?)(?=\n[ABCD]\.|\nAnswer:|\Z)",
        re.MULTILINE | re.DOTALL | re.IGNORECASE
    )

    matches = list(option_pattern.finditer(original_prompt))

    if not matches:
        # No options found, return original prompt and empty map
        return original_prompt, {}

    original_options_data = []
    for match in matches:
        original_options_data.append({
            "label": match.group(1).upper(),
            "text": match.group(2).strip(),
            "start": match.start(),
            "end": match.end()
        })

    # Filter out excluded options
    remaining_options = [
        opt for opt in original_options_data if opt["label"] not in exclude_options
    ]

    if not remaining_options:
         # Handle case where all options are excluded (optional: raise error or return specific state)
         print("Warning: All options were excluded.")
         # Return prompt up to where options started, plus "Answer:", and empty map
         options_start_index = original_options_data[0]['start']
         prefix = original_prompt[:options_start_index].strip()
         return prefix + "\nAnswer:", {}


    # Re-label remaining options and build the mapping
    new_options_list = []
    new_to_original_map = {}
    new_labels = "ABCDEFGHIJKLMNOPQRSTUVWXYZ" # Sufficient labels

    for i, option_data in enumerate(remaining_options):
        new_label = new_labels[i]
        new_options_list.append(f"{new_label}. {option_data['text']}")
        new_to_original_map[new_label] = option_data['label']

    # Reconstruct the prompt string
    # Find the start of the first option and the start of the "Answer:" section
    options_start_index = original_options_data[0]['start']
    # Find where the original options block ends (end of the last original match)
    options_end_index = original_options_data[-1]['end']

    prefix = original_prompt[:options_start_index].strip()
    # Find the "Answer:" part, assuming it comes after the options
    answer_part_match = re.search(r"\nAnswer:", original_prompt[options_end_index:], re.IGNORECASE)
    suffix = "\nAnswer:" # Default suffix if the original wasn't found after options

    if answer_part_match:
         # If original "Answer:" exists, use it to ensure correct spacing/structure
         # We deliberately exclude the "After consideration..." part here.
         suffix = "\nAnswer:" # Keep it clean

    modified_prompt = prefix + "\n" + "\n".join(new_options_list) + suffix

    return modified_prompt, new_to_original_map


def map_answer_to_original(generated_answer: str, new_to_original_map: dict[str, str]) -> str:
    """
    Maps a generated answer (corresponding to a re-labeled option) back
    to its original option label using the provided mapping.

    Args:
        generated_answer: The answer label generated by the model (e.g., 'A', 'B').
        new_to_original_map: The dictionary mapping new labels back to original labels,
                             as returned by modify_question_options.

    Returns:
        The original option label corresponding to the generated answer.
        Returns the generated_answer itself if it's not found in the map (fallback).
    """
    # Clean up potential extra characters around the answer
    cleaned_answer = generated_answer.strip().upper()
    # Basic extraction if it's like "(A)" or "A."
    match = re.match(r'\(?([A-Z])\)?\.?', cleaned_answer)
    if match:
        cleaned_answer = match.group(1)

    return new_to_original_map.get(cleaned_answer, generated_answer) # Use .get for safe fallback


def extract_multiple_choice_answer(llm_output: str, options: str = "ABCD") -> str | None:
    """
    从大模型的输出文本中提取多项选择题的答案 (A, B, C, D 等)。

    考虑到 LLM 可能的输出格式，例如：
    - "A"
    - "B."
    - "答案是 C"
    - "选择 D"
    - "The correct option is A."
    - "I choose B"
    - "**C**"
    - "(D)"
    - "A. ..." (只取 A)
    - "我觉得是 B"
    - "选项 C 是正确的"
    - 等等

    Args:
        llm_output (str): 大模型生成的包含答案的文本。
        options (str): 可能的选项字母字符串，默认为 "ABCD"。

    Returns:
        str | None: 提取到的选项字母 (大写)，如果未能明确提取则返回 None。
    """
    if not llm_output:
        return None

    # 0. 预处理：去除首尾空格，统一处理大小写可能有助于匹配，但选项本身需要区分大小写，所以暂时只strip
    text = llm_output.strip()
    if not text:
        return None

    # 1. 优先匹配明确的答案指示词 + 选项 (修正版)
    #    例如："答案是 A", "选择 B", "Option: C", "选 D", "Answer is D"
    #    (?:...) 表示非捕获组
    #    \s*[:：]?\s* 匹配可选的空格、可选的冒号（中英文）、可选的空格
    #    (?:是|is)?\s* 匹配可选的 "是" 或 "is"、可选的空格
    #    ([{options}]) 捕获选项字母
    #    \b 确保选项字母是独立的单词边界（防止匹配单词内部的字母）
    pattern_explicit = rf"(?:答案|选项|选择|answer|option|choice|choose|select)\s*[:：]?\s*(?:是|is)?\s*([{options}])\b"
    match = re.search(pattern_explicit, text, re.IGNORECASE)
    if match:
        # 提取捕获组 1 (选项字母) 并转为大写
        return match.group(1).upper()

    # 2. 匹配被括号、星号等包围的单个选项字母
    #    例如："(A)", "[B]", "**C**", "*D*", "{A}"
    #    (?<!\w) 和 (?!\w) 用于确保选项字母前后不是其他字母数字字符（比 \b 更灵活）
    #    (?:...) 包裹各种可能的包围符号
    pattern_bracketed = rf"(?:[\[({{*<])\s*([{options}])\s*(?:[\])}}*>.])"
    match = re.search(pattern_bracketed, text, re.IGNORECASE)
    if match:
        return match.group(1).upper()

    # 尝试宽松一点，匹配前后有非字母数字字符或边界的单个选项
    # 改进：使用更严格的边界检查，避免匹配 "Grade A"
    # 要求前面是开头、空格或特定标点，后面是结尾、空格或特定标点
    pattern_isolated_boundary = rf"(?:^|\s|[:：(\[{{*<])\s*([{options}])\s*(?:$|\s|[.,!?)\]}}*>:])"
    match = re.search(pattern_isolated_boundary, text, re.IGNORECASE)
    if match:
        return match.group(1).upper()

    # 3. 匹配以选项字母 + 点/括号/空格 开头的字符串
    #    例如："A. xxx", "B) yyy", "C zzz"
    #    ^ 表示字符串开头
    #    \s* 匹配开头的可选空格
    #    使用 \b 确保选项字母后不是其他字母 (防止匹配 "Apple")
    pattern_start = rf"^\s*([{options}])\b(?:\.|\)|\s|:|$)"
    match = re.match(pattern_start, text, re.IGNORECASE) # 使用 match 确保从头开始
    if match:
        return match.group(1).upper()

    # 4. 匹配句子中 "is A", "是 B" 这样的结构 (优先级较低)
    #    确保 "is" 或 "是" 前面是一个单词边界
    pattern_is_option = rf"\b(?:is|是)\s+([{options}])\b"
    match = re.search(pattern_is_option, text, re.IGNORECASE)
    if match:
        return match.group(1).upper()

    # 5. 最后手段：检查整个字符串是否仅仅是选项字母（可能带标点）
    #    去除所有标点和空格后，看是否只剩下一个在 options 中的字母
    cleaned_text = ''.join(c for c in text if c not in string.punctuation and not c.isspace())
    if len(cleaned_text) == 1 and cleaned_text.upper() in options.upper():
         return cleaned_text.upper()

    # 6. 如果以上策略都失败，则无法确定答案
    return None


class LLaVAVideo_ICL(BaseModel):
    INSTALL_REQ = True
    INTERLEAVE = True
    VIDEO_LLM = True
    DEFAULT_IMAGE_TOKEN = "<image>"
    IMAGE_TOKEN_INDEX = -200

    def __init__(
            self,
            model_path="/mnt/afs/wangkaibin/models/LLaVA-Video-7B-Qwen2",
            model_config=None,
            **kwargs
    ):
        assert model_path is not None
        try:
            from llava.model.builder import load_pretrained_model
            from llava.conversation import conv_templates, SeparatorStyle
            from llava.mm_utils import (
                get_model_name_from_path,
                process_images,
                tokenizer_image_token,
                KeywordsStoppingCriteria,
            )  # noqa: E501
        except Exception as err:
            logging.critical(
                "Please `pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git`"
            )
            raise err

        self.model_config = model_config

        video_kwargs_default = dict(
            overwrite=True, mm_spatial_pool_mode="average", force_sample=True
        )
        video_kwargs_default.update(kwargs)
        self.video_kwargs = video_kwargs_default

        overwrite_config = None
        if "video" in model_path.lower():
            if self.video_kwargs["overwrite"]:
                overwrite_config = {}
                overwrite_config["mm_spatial_pool_mode"] = self.video_kwargs[
                    "mm_spatial_pool_mode"
                ]

        rank, world_size = get_rank_and_world_size()
        model_name = get_model_name_from_path(model_path)
        if model_name == "LLaVA-Video-7B-Qwen2":
            model_name = "llava_qwen"
        import warnings
        # filter warning align with official code
        warnings.filterwarnings("ignore")
        tokenizer, model, image_processor, _ = load_pretrained_model(
            model_path,
            None,
            model_name,
            device_map=0,
            overwrite_config=overwrite_config,
            model_config=model_config
        )
        model.eval()
        model.tie_weights()

        if "llava" in model_path.lower():
            conv_mode = "qwen_1_5"
        if 'llava-video' in model_path.lower():
            self.nframe = 64
        else:
            self.nframe = 16
            if "72b" in model_path.lower():
                self.nframe = 32

        if "video" in model_path.lower():
            self.force_sample = self.video_kwargs["force_sample"]
        else:
            self.force_sample = False

        self.conv_template = conv_mode
        self.conv_templates = conv_templates
        self.tokenizer = tokenizer
        self.model = model
        self.image_processor = image_processor
        self.tokenizer_image_token = tokenizer_image_token
        self.process_images = (
            process_images  # Store process_images as a class attribute
        )
        self.KeywordStoppingCriteria = KeywordsStoppingCriteria
        self.SeparatorStyle = SeparatorStyle

    def generate_inner_image(self, message, dataset=None):
        content, images = "", []
        image_sizes = []  # Store image sizes

        for msg in message:
            if msg["type"] == "text":
                content += msg["value"]
            else:
                img = Image.open(msg["value"]).convert("RGB")
                images.append(img)
                image_sizes.append(img.size)  # Store the size of each image
                content += self.DEFAULT_IMAGE_TOKEN + "\n"

        # Process images using the class attribute self.process_images
        image_tensor = self.process_images(
            images, self.image_processor, self.model.config
        )
        image_tensor = [
            _image.to(dtype=torch.float16, device="cuda") for _image in image_tensor
        ]

        conv = copy.deepcopy(self.conv_templates[self.conv_template])
        conv.append_message(conv.roles[0], content)
        conv.append_message(conv.roles[1], None)
        prompt_question = conv.get_prompt()

        input_ids = self.tokenizer_image_token(
            prompt_question, self.tokenizer, self.IMAGE_TOKEN_INDEX, return_tensors="pt"
        )
        input_ids = input_ids.unsqueeze(0).cuda()

        stop_str = conv.sep if conv.sep_style != self.SeparatorStyle.TWO else conv.sep2
        keywords = [stop_str]
        stopping_criteria = self.KeywordStoppingCriteria(
            keywords, self.tokenizer, input_ids
        )

        # Pass image sizes along with other parameters
        cont = self.model.generate(
            input_ids,
            images=image_tensor,
            image_sizes=image_sizes,  # Pass the image sizes here
            do_sample=False,
            temperature=0,
            max_new_tokens=2048,
            stopping_criteria=[stopping_criteria],
        )
        text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)[0]
        return text_outputs

    def generate_inner_video(self, message, dataset=None):
        content, text_content, visual_content, videos = "", "", "", []

        for msg in message:
            if msg["type"] == "text":
                text_content += msg["value"]
            else:
                videos.append(msg["value"])
                visual_content += self.DEFAULT_IMAGE_TOKEN + "\n"

        if len(videos) > 1:
            raise ValueError(
                "LLaVA-OneVision does not support multiple videos as input."
            )

        video_frames, frame_time, video_time = self.load_video(
            videos[0], self.nframe, 1, self.force_sample
        )
        frame_time = frame_time.split(',')

        vote = {key: 0 for key in "ABCD"}

        def generate_chunk_answer(chunk_begin, chunk_end):
            video_frames_chunk = video_frames[chunk_begin:chunk_end]
            frame_time_chunk = ','.join(frame_time[chunk_begin:chunk_end])

            time_instruciton = (
                f"The video lasts for {video_time:.2f} seconds,"
                f"and {len(video_frames_chunk)} frames are uniformly sampled from it."
                f"These frames are located at {frame_time_chunk}."
                f"Please answer the following questions related to this video.\n"
            )

            if self.force_sample:
                content = visual_content + time_instruciton + text_content
            else:
                content = visual_content + text_content

            image_tensors = []
            frames = (
                self.image_processor.preprocess(video_frames_chunk, return_tensors="pt")[
                    "pixel_values"
                ]
                .half()
                .cuda()
            )
            image_tensors.append(frames)

            conv = copy.deepcopy(self.conv_templates[self.conv_template])
            conv.append_message(conv.roles[0], content)
            conv.append_message(conv.roles[1], None)
            prompt_question = conv.get_prompt()

            input_ids = self.tokenizer_image_token(
                prompt_question, self.tokenizer, self.IMAGE_TOKEN_INDEX, return_tensors="pt"
            )
            input_ids = input_ids.unsqueeze(0).cuda()
            image_sizes = [frame.size for frame in video_frames]
            modalities = ["video"] * len(video_frames)

            stop_str = conv.sep if conv.sep_style != self.SeparatorStyle.TWO else conv.sep2
            keywords = [stop_str]
            stopping_criteria = self.KeywordStoppingCriteria(
                keywords, self.tokenizer, input_ids
            )

            # Pass image sizes along with other parameters
            cont = self.model.generate(
                input_ids,
                images=image_tensors,
                image_sizes=image_sizes,  # Pass the image sizes here
                do_sample=False,
                temperature=0,
                max_new_tokens=4,
                modalities=modalities,
                stopping_criteria=[stopping_criteria],
            )
            text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)[0]
            answer = extract_multiple_choice_answer(text_outputs)
            return answer

        for chunk_begin in range(0, len(video_frames), self.model_config.frame_per_chunk):
            chunk_end = min(len(video_frames), chunk_begin + self.model_config.frame_per_chunk)
            answer = generate_chunk_answer(chunk_begin, chunk_end)
            if answer in vote:
                vote[answer] += 1
        answer = generate_chunk_answer(0, len(video_frames))
        if answer in vote:
            vote[answer] += 1

        exclude_answers = [key for key, value in vote.items() if value == 0]
        breakpoint()

        time_instruciton = (
            f"The video lasts for {video_time:.2f} seconds,"
            f"and {len(video_frames)} frames are uniformly sampled from it."
            f"These frames are located at {','.join(frame_time)}."
            f"Please answer the following questions related to this video.\n"
        )

        if self.force_sample:
            content = visual_content + time_instruciton + text_content
        else:
            content = visual_content + text_content
        content, mapping = modify_question_options(content, exclude_answers)

        image_tensors = []
        frames = (
            self.image_processor.preprocess(video_frames, return_tensors="pt")[
                "pixel_values"
            ]
            .half()
            .cuda()
        )
        image_tensors.append(frames)

        conv = copy.deepcopy(self.conv_templates[self.conv_template])
        conv.append_message(conv.roles[0], content)
        conv.append_message(conv.roles[1], None)
        prompt_question = conv.get_prompt().replace(" (A, B, C, or D)", "")

        input_ids = self.tokenizer_image_token(
            prompt_question, self.tokenizer, self.IMAGE_TOKEN_INDEX, return_tensors="pt"
        )
        input_ids = input_ids.unsqueeze(0).cuda()
        image_sizes = [frame.size for frame in video_frames]
        modalities = ["video"] * len(video_frames)

        stop_str = conv.sep if conv.sep_style != self.SeparatorStyle.TWO else conv.sep2
        keywords = [stop_str]
        stopping_criteria = self.KeywordStoppingCriteria(
            keywords, self.tokenizer, input_ids
        )

        allowed_next_words = ["A", "B", "C", "D"]
        allowed_next_token_ids = self.tokenizer.convert_tokens_to_ids(allowed_next_words)
        prompt_length = input_ids.shape[-1]
        def force_next_token_from_list(batch_id, input_ids):
            current_length = input_ids.shape[-1]
            if current_length == prompt_length:
                return allowed_next_token_ids
            else:
                return list(range(self.tokenizer.vocab_size))

        cont = self.model.generate(
            input_ids,
            images=image_tensors,
            image_sizes=image_sizes,
            do_sample=False,
            temperature=0,
            max_new_tokens=1,
            modalities=modalities,
            stopping_criteria=[stopping_criteria],
            prefix_allowed_tokens_fn=force_next_token_from_list,
        )
        text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)[0]
        print(prompt_question, text_outputs)
        answer = extract_multiple_choice_answer(text_outputs)
        if answer is None:
            print(f'#### answer is None: \n{prompt_question}\n{text_outputs}')
            answer = 'A'
        answer = map_answer_to_original(answer, mapping)
        breakpoint()

        return answer

    # def generate_inner_video(self, message, dataset=None):
    #     content, text_content, visual_content, videos = "", "", "", []

    #     for msg in message:
    #         if msg["type"] == "text":
    #             text_content += msg["value"]
    #         else:
    #             videos.append(msg["value"])
    #             visual_content += self.DEFAULT_IMAGE_TOKEN + "\n"

    #     if len(videos) > 1:
    #         raise ValueError(
    #             "LLaVA-OneVision does not support multiple videos as input."
    #         )

    #     video_frames, frame_time, video_time = self.load_video(
    #         videos[0], self.nframe, 1, self.force_sample
    #     )
    #     frame_time = frame_time.split(',')

    #     vote = {key: 0 for key in "ABCD"}

    #     for chunk_begin in range(0, len(video_frames), self.model_config.frame_per_chunk):
    #         chunk_end = min(len(video_frames), chunk_begin + self.model_config.frame_per_chunk)
    #         video_frames_chunk = video_frames[chunk_begin:chunk_end]
    #         frame_time_chunk = ','.join(frame_time[chunk_begin:chunk_end])

    #         time_instruciton = (
    #             f"The video lasts for {video_time:.2f} seconds,"
    #             f"and {len(video_frames_chunk)} frames are uniformly sampled from it."
    #             f"These frames are located at {frame_time_chunk}."
    #             f"Please answer the following questions related to this video.\n"
    #         )

    #         if self.force_sample:
    #             content = visual_content + time_instruciton + text_content
    #         else:
    #             content = visual_content + text_content

    #         image_tensors = []
    #         frames = (
    #             self.image_processor.preprocess(video_frames_chunk, return_tensors="pt")[
    #                 "pixel_values"
    #             ]
    #             .half()
    #             .cuda()
    #         )
    #         image_tensors.append(frames)

    #         conv = copy.deepcopy(self.conv_templates[self.conv_template])
    #         conv.append_message(conv.roles[0], content)
    #         conv.append_message(conv.roles[1], None)
    #         prompt_question = conv.get_prompt()

    #         input_ids = self.tokenizer_image_token(
    #             prompt_question, self.tokenizer, self.IMAGE_TOKEN_INDEX, return_tensors="pt"
    #         )
    #         input_ids = input_ids.unsqueeze(0).cuda()
    #         image_sizes = [frame.size for frame in video_frames]
    #         modalities = ["video"] * len(video_frames)

    #         stop_str = conv.sep if conv.sep_style != self.SeparatorStyle.TWO else conv.sep2
    #         keywords = [stop_str]
    #         stopping_criteria = self.KeywordStoppingCriteria(
    #             keywords, self.tokenizer, input_ids
    #         )

    #         # Pass image sizes along with other parameters
    #         cont = self.model.generate(
    #             input_ids,
    #             images=image_tensors,
    #             image_sizes=image_sizes,  # Pass the image sizes here
    #             do_sample=False,
    #             temperature=0,
    #             max_new_tokens=4,
    #             modalities=modalities,
    #             stopping_criteria=[stopping_criteria],
    #         )
    #         text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)[0]
    #         answer = extract_multiple_choice_answer(text_outputs)
    #         if answer in vote:
    #             vote[answer] += 1

    #     exclude_answers = [key for key, value in vote.items() if value == 0]
    #     if len(exclude_answers) > 0:
    #         exclude_answers = ",".join(exclude_answers)
    #         thought = f"After consideration, options {exclude_answers} can be ruled out. So the answer is"
    #     else:
    #         thought = ""

    #     time_instruciton = (
    #         f"The video lasts for {video_time:.2f} seconds,"
    #         f"and {len(video_frames)} frames are uniformly sampled from it."
    #         f"These frames are located at {','.join(frame_time)}."
    #         f"Please answer the following questions related to this video.\n"
    #     )

    #     if self.force_sample:
    #         content = visual_content + time_instruciton + text_content
    #     else:
    #         content = visual_content + text_content
    #     content = content + thought

    #     image_tensors = []
    #     frames = (
    #         self.image_processor.preprocess(video_frames_chunk, return_tensors="pt")[
    #             "pixel_values"
    #         ]
    #         .half()
    #         .cuda()
    #     )
    #     image_tensors.append(frames)

    #     conv = copy.deepcopy(self.conv_templates[self.conv_template])
    #     conv.append_message(conv.roles[0], content)
    #     conv.append_message(conv.roles[1], None)
    #     prompt_question = conv.get_prompt()

    #     input_ids = self.tokenizer_image_token(
    #         prompt_question, self.tokenizer, self.IMAGE_TOKEN_INDEX, return_tensors="pt"
    #     )
    #     input_ids = input_ids.unsqueeze(0).cuda()
    #     image_sizes = [frame.size for frame in video_frames]
    #     modalities = ["video"] * len(video_frames)

    #     stop_str = conv.sep if conv.sep_style != self.SeparatorStyle.TWO else conv.sep2
    #     keywords = [stop_str]
    #     stopping_criteria = self.KeywordStoppingCriteria(
    #         keywords, self.tokenizer, input_ids
    #     )

    #     breakpoint()
    #     # Pass image sizes along with other parameters
    #     cont = self.model.generate(
    #         input_ids,
    #         images=image_tensors,
    #         image_sizes=image_sizes,  # Pass the image sizes here
    #         do_sample=False,
    #         temperature=0,
    #         max_new_tokens=4,
    #         modalities=modalities,
    #         stopping_criteria=[stopping_criteria],
    #     )
    #     text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)[0]

    #     print('####################################')
    #     print(prompt_question)
    #     print(text_outputs)

    #     return text_outputs

    def load_video(self, video_path, max_frames_num, fps=1, force_sample=False):
        from decord import VideoReader, cpu
        import numpy as np

        if max_frames_num == 0:
            return np.zeros((1, 336, 336, 3))
        vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)
        total_frame_num = len(vr)
        video_time = total_frame_num / vr.get_avg_fps()
        fps = round(vr.get_avg_fps() / fps)
        frame_idx = [i for i in range(0, len(vr), fps)]
        frame_time = [i / fps for i in frame_idx]
        if len(frame_idx) > max_frames_num or force_sample:
            sample_fps = max_frames_num
            uniform_sampled_frames = np.linspace(
                0, total_frame_num - 1, sample_fps, dtype=int
            )
            frame_idx = uniform_sampled_frames.tolist()
            frame_time = [i / vr.get_avg_fps() for i in frame_idx]
        frame_time = ",".join([f"{i:.2f}s" for i in frame_time])
        spare_frames = vr.get_batch(frame_idx).asnumpy()
        # import pdb;pdb.set_trace()
        return spare_frames, frame_time, video_time

    def generate_inner(self, message, dataset=None):
        if DATASET_MODALITY(dataset) == 'VIDEO':
            return self.generate_inner_video(message, dataset)
        else:
            return self.generate_inner_image(message, dataset)
